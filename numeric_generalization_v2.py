__author__ = 'olivia'

import sys, pickle
import numpy as np
import pandas as pd
from de_id_functions import *

"""
Bin a set of numeric values so that at least n entities are within each bin. In particular,
this code will take the Year of Birth (YoB) and the number of forum posts (nforum_posts) values
and produce bins with a particular range and calculate the mean for that range.  This code is
improved in order to bin values together in such a way that minimizes the distortion of the mean
of the post-binned values.

The value '9999' (or '9999.0') is used as a marked value to show that there is nothing associated
in the record. For YoB, this value is not recorded in the resulting table of intervals and
means. For nforum posts, this value is included in the table, given an interval which includes
the final interval for the posts, but should be treated specially when calculating suppression
sets for anonymity and when producing the final, de-identified data file. In those cases,
the value of '9999.0' should be caught specially and replaced with '0'.
"""

YoB_binsize = 25000
nforum_post_binsize = 25000

k = 5

# Working for year of birth and number of forum posts
def collapse(val,num,denom,maxbinsize):
    """
    Given a max bin size, greedily find the endpoints that will create the bins with the smallest distortion to the
    overall bin mean that has at least maxbinsize members.
    Note that this only works for integer values. The function also finds the mean of the values in each bin.

    :param val: a list of the original, unbinned values
    :param num: a list of the sum of the values with the new binned values
    :param denom: a list of the number of values with the new binned values
    :return: three lists of val, num, denom where val is a list of lists of the original values that now belong
    to each bin; num is the sum of all the values that belong in that bin; and denom is the count of the number
    of values that belong to that bin.
    """
    def left_merge(index,val=val,num=num,denom=denom):
        num[index-1] = num[index-1] + num[index]
        num = np.delete(num,index)
        denom[index-1] = denom[index-1] + denom[index]
        denom = np.delete(denom,index)
        val[index-1] = val[index-1]+val[index]
        del val[index]
        return val,num,denom
    
    def right_merge(index,val=val,num=num,denom=denom):
        num[index] = num[index] + num[index+1]
        num = np.delete(num,index+1)
        denom[index] = denom[index] + denom[index+1]
        denom = np.delete(denom,index+1)
        val[index] = val[index]+val[index+1]
        del val[index+1]
        return val,num,denom
    
    indices_less_than_k = np.where(denom<maxbinsize)[0]
    left_indices = indices_less_than_k-1
    # if left endpoint is in list, then don't calculate for that index
    if -1 not in left_indices:
        left_errors = abs(num[indices_less_than_k]/denom[indices_less_than_k] - num[left_indices]/denom[left_indices])
        minerror_left = min(left_errors)
        minerror_left_index = indices_less_than_k[np.argmin(left_errors)]
    elif len(left_indices) > 1:
        # compare with left bucket
        left_errors = abs(num[indices_less_than_k[1:]]/denom[indices_less_than_k[1:]] - num[left_indices[1:]]/denom[left_indices[1:]])
        minerror_left = min(left_errors)
        minerror_left_index = indices_less_than_k[np.argmin(left_errors)+1]
    else:
        minerror_left = np.inf
        minerror_left_index = np.inf
        

    right_indices = indices_less_than_k+1
    # if right endpoint is in list, then don't calculate for that index
    if len(denom) not in right_indices:
        right_errors = abs(num[indices_less_than_k]/denom[indices_less_than_k] - num[right_indices]/denom[right_indices])
        minerror_right = min(right_errors)
        minerror_right_index = indices_less_than_k[np.argmin(right_errors)]
    elif len(right_indices)>1:
        # compare with right bucket
        right_errors = abs(num[indices_less_than_k[:-1]]/denom[indices_less_than_k[:-1]] - num[right_indices[:-1]]/denom[right_indices[:-1]])
        minerror_right = min(right_errors)
        minerror_right_index = indices_less_than_k[np.argmin(right_errors)]
    else:
        minerror_right = np.inf
        minerror_right_index = np.inf
    
    # overall smallest error
    # collapse left
    if minerror_left <= minerror_right:
        return left_merge(minerror_left_index)
    elif minerror_right < minerror_left:
        return right_merge(minerror_right_index)


# Creates a dictionary that maps each unique value onto a corresponding range
def createConversionDict(val,num,denom,numdenom,numDictName):
    """
    Take a list of endpoints as generated by collapse and create a dictionary whose keys are the unique first
    values in qry and whose corresponding values are a pair of the bin that each of the unique values in the dataset
    should be mapped onto and the mean of that bin.

    :param val: a list of the original, unbinned values
    :param num: a list of the sum of the values with the new binned values
    :param denom: a list of the number of values with the new binned values
    :param numdenom: a pandas dataframe of the original values, counts, and sums
    :return: a dictionary keyed by value binned with values the pair (bin range, mean). Bin range will be a
        string, while mean will be a floating point value
    """
    import pickle
    numDict = {}
    for i,v in enumerate(val):
        tm = numdenom.ix[v]
        bucket_mean = sum(tm['count'] * tm['value']) / float(tm['count'].sum())
        for v2 in v:
            if len(v)==1:
                numDict[v2] = (str(v[0]), bucket_mean)
            else:
                numDict[v2] = (str(min(v))+'-'+str(max(v)), bucket_mean)
    return numDict


def main(c, year_bin_file, post_bin_file):
    table = 'source'
    global qry, endpts, year_conversion, nforumposts_conversion
    ########################################################
    # Bin years of birth
    c.execute("SELECT YoB, COUNT(*) as \'num\', SUM(YoB) as \'sum\' FROM " + table + " WHERE YoB > 0 GROUP BY YoB ORDER BY YoB")
    qry = c.fetchall()

    try:
        qry = [(int(float(z[0])), int(z[1]), int(z[2])) for z in qry]  # convert string floats to ints in qry count
    except:
        print "Year of birth list contains invalid entry"
        pass
    # Process data into pandas dataframe
    numdenom = pd.DataFrame(qry)
    numdenom.columns = ['value', 'count', 'sum']
    numdenom.index = numdenom['value']
    numdenom['value'] = [int(float(x)) if x!='nan' else 9999 for x in numdenom['value']]
    numdenom.index = list([int(float(x)) for x in numdenom.index[:len(numdenom)-1]]) + [9999]
    numdenom = numdenom.sort_values('value')
    # create three lists
    val = [[x] for x in list(numdenom['value'].values)]
    num = numdenom['sum'].values
    denom = numdenom['count'].values
    # Bin year of birth
    while sum(denom<k)>1:
        val,num,denom = collapse(val,num,denom,YoB_binsize)
    # Create dictionary that maps every unique value to a corresponding bin, and add the empty values to it
    dictyob = createConversionDict(val,num,denom,numdenom,year_bin_fname)
    pickle.dump(dictyob,year_bin_file)

    ########################################################
    # Bin number of forum posts
    # Replace all values of nforum_posts that are blank with a temporary 9999
    c.execute("SELECT nforum_posts, COUNT(*) as \'num\', SUM(nforum_posts) as \'sum\' FROM " + table + " GROUP BY nforum_posts ORDER BY nforum_posts")
    qry = c.fetchall()
    try:
        qry = [(int(z[0]), int(z[1]), int(z[2])) for z in qry]  # convert string floats to ints in qry count
    except:
        print "Number of forum posts includes an invalid entry"
        pass
    # Process data into pandas dataframe
    numdenom = pd.DataFrame(qry)
    numdenom.columns = ['value', 'count', 'sum']
    numdenom.index = numdenom['value']
    numdenom['value'] = [int(float(x)) if x!='nan' else 9999 for x in numdenom['value']]
    numdenom.index = list([int(float(x)) for x in numdenom.index[:len(numdenom)-1]]) + [9999]
    numdenom = numdenom.sort_values('value')
    # create three lists
    val = [[x] for x in list(numdenom['value'].values)]
    num = numdenom['sum'].values
    denom = numdenom['count'].values
    # Bin year of birth
    while sum(denom<k)>1:
        val,num,denom = collapse(val,num,denom,nforum_post_binsize)
    # Create dictionary that maps every unique value to a corresponding bin, and add the empty values to it
    dictpost = createConversionDict(val,num,denom,numdenom,post_bin_fname)
    pickle.dump(dictpost,post_bin_file)

if __name__ == '__main__':
    """
    The main routine, that will take as input the database name of the
    untouched database and output pickle files with the generalization
    mappings of the most greedily optimal bins that optimize for
    minimal distortion in the mean of the bin.
    """
    if len(sys.argv) < 4:
        print 'Usage: python numeric_generalization_v2.py dbName yearBinFileName postBinFileName'
        sys.exit(1)

    dbname = sys.argv[1]
    cur = dbOpen(dbname)
    year_bin_fname = sys.argv[2]
    year_bin_file = open(year_bin_fname, 'w')
    post_bin_fname = sys.argv[3]
    post_bin_file = open(post_bin_fname, 'w')
    if len(sys.argv) > 4:
        try:
            YoB_binsize = int(sys.argv[4])
            print 'building bins for YoB with size ', str(YoB_binsize)
        except:
            print'Invalid argument for Year of Birth bin size; value must be an integer'
    if len(sys.argv) > 5:
        try:
            nforum_post_binsize = int(sys.argv[5])
            print 'building bins for forum posts with size ', str(nforum_post_binsize)
        except:
            print 'invalid argument for forum bin size, value must be an integer'

    main(cur, year_bin_file, post_bin_file)
    dbClose(cur)