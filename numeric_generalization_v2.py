__author__ = 'olivia'

import sys, pickle
import numpy as np
import pandas as pd
import csv
#from de_id_functions import *

"""
Bin a set of numeric values so that at least n entities are within each bin. In particular,
this code will take the Year of Birth (YoB) and the number of forum posts (nforum_posts) values
and produce bins with a particular range and calculate the mean for that range.  This code is
improved in order to bin values together in such a way that minimizes the distortion of the mean
of the post-binned values.

The value '9999' (or '9999.0') is used as a marked value to show that there is nothing associated
in the record. For YoB, this value is not recorded in the resulting table of intervals and
means. For nforum posts, this value is included in the table, given an interval which includes
the final interval for the posts, but should be treated specially when calculating suppression
sets for anonymity and when producing the final, de-identified data file. In those cases,
the value of '9999.0' should be caught specially and replaced with '0'.
"""

YoB_binsize = 25000
nforum_post_binsize = 25000

k = 5

# Working for year of birth and number of forum posts
def collapse(val,num,denom,maxbinsize):
    """
    Given a max bin size, greedily find the endpoints that will create the bins with the smallest distortion to the
    overall bin mean that has at least maxbinsize members.
    Note that this only works for integer values. The function also finds the mean of the values in each bin.

    :param val: a list of the original, unbinned values
    :param num: a list of the sum of the values with the new binned values
    :param denom: a list of the number of values with the new binned values
    :return: three lists of val, num, denom where val is a list of lists of the original values that now belong
    to each bin; num is the sum of all the values that belong in that bin; and denom is the count of the number
    of values that belong to that bin.
    """
    def left_merge(index,val=val,num=num,denom=denom):
        num[index-1] = num[index-1] + num[index]
        num = np.delete(num,index)
        denom[index-1] = denom[index-1] + denom[index]
        denom = np.delete(denom,index)
        val[index-1] = val[index-1]+val[index]
        del val[index]
        return val,num,denom
    
    def right_merge(index,val=val,num=num,denom=denom):
        num[index] = num[index] + num[index+1]
        num = np.delete(num,index+1)
        denom[index] = denom[index] + denom[index+1]
        denom = np.delete(denom,index+1)
        val[index] = val[index]+val[index+1]
        del val[index+1]
        return val,num,denom
    
    indices_less_than_k = np.where(denom<maxbinsize)[0]
    left_indices = indices_less_than_k-1
    # if left endpoint is in list, then don't calculate for that index
    if -1 not in left_indices:
        left_errors = abs(num[indices_less_than_k]/denom[indices_less_than_k] - num[left_indices]/denom[left_indices])
        minerror_left = min(left_errors)
        minerror_left_index = indices_less_than_k[np.argmin(left_errors)]
    elif len(left_indices) > 1:
        # compare with left bucket
        left_errors = abs(num[indices_less_than_k[1:]]/denom[indices_less_than_k[1:]] - num[left_indices[1:]]/denom[left_indices[1:]])
        minerror_left = min(left_errors)
        minerror_left_index = indices_less_than_k[np.argmin(left_errors)+1]
    else:
        minerror_left = np.inf
        minerror_left_index = np.inf
        

    right_indices = indices_less_than_k+1
    # if right endpoint is in list, then don't calculate for that index
    if len(denom) not in right_indices:
        right_errors = abs(num[indices_less_than_k]/denom[indices_less_than_k] - num[right_indices]/denom[right_indices])
        minerror_right = min(right_errors)
        minerror_right_index = indices_less_than_k[np.argmin(right_errors)]
    elif len(right_indices)>1:
        # compare with right bucket
        right_errors = abs(num[indices_less_than_k[:-1]]/denom[indices_less_than_k[:-1]] - num[right_indices[:-1]]/denom[right_indices[:-1]])
        minerror_right = min(right_errors)
        minerror_right_index = indices_less_than_k[np.argmin(right_errors)]
    else:
        minerror_right = np.inf
        minerror_right_index = np.inf
    
    # overall smallest error
    # collapse left
    if minerror_left <= minerror_right:
        return left_merge(minerror_left_index)
    elif minerror_right < minerror_left:
        return right_merge(minerror_right_index)


# Creates a dictionary that maps each unique value onto a corresponding range
def createConversionDict(val,num,denom,numdenom):
    """
    Take a list of endpoints as generated by collapse and create a dictionary whose keys are the unique first
    values in qry and whose corresponding values are a pair of the bin that each of the unique values in the dataset
    should be mapped onto and the mean of that bin.

    :param val: a list of the original, unbinned values
    :param num: a list of the sum of the values with the new binned values
    :param denom: a list of the number of values with the new binned values
    :param numdenom: a pandas dataframe of the original values, counts, and sums
    :return: a dictionary keyed by value binned with values the pair (bin range, mean). Bin range will be a
        string, while mean will be a floating point value
    """
    numDict = {}
    for i,v in enumerate(val):
        tm = numdenom.ix[v]
        #bucket_mean = sum(tm['count'] * tm['item']) / float(tm['count'].sum())
        #bucket_count = tm['count'].sum()
        bucket_mean = tm['sum'].sum() / float(tm['count'].sum())
        for v2 in v:
            if len(v)==1:
                numDict[v2] = (str(v[0]), bucket_mean)
            else:
                numDict[v2] = (str(min(v))+'-'+str(max(v)), bucket_mean)
    return numDict

def build_bins(val_l):
    l_end = val_l.pop()
    if l_end[0] != '':
        val_l.append(l_end)
        l_end = None
    d_frame = pd.DataFrame(val_l)
    d_frame.columns = ['item', 'count', 'sum']
    d_frame.index = d_frame['item']
    val = [[x] for x in list(d_frame['item'].values)]
    denom = d_frame['count'].values
    num = d_frame['sum'].values
    while sum(denom<k) > 1:
        val, num, denom = collapse(val, num, denom, YoB_binsize)
    dictobj = createConversionDict(val, num, denom, d_frame)
    if l_end != None:
        dictobj[''] = l_end[1]
    return dictobj

def update_num_dict(val, dict):
    if val != '':
        i = int(val)
    else:
        i = val
    if i in dict:
        dict[i][0] += 1
        dict[i][1] += i
    else:
        dict[i] = [1, i]

    return dict

def dict_to_list(d):
    ret_list = []
    for i in d.iterkeys():
        ret_list.append([i, d[i][0], d[i][1]])
    ret_list.sort()
    l = ret_list.pop()
    if l[0] == '':
        l[2] = 0
    ret_list.append(l)
    return ret_list

def dump_map(m, f_name):
    f_out = open(f_name, 'w')
    pickle.dump(m, f_out)
    f_out.close()
    return

def create_value_maps(cin, fname_ps):

    #create the dictionaries for each of the different numeric values
    yob_d = {}
    f_post_d = {}
    f_votes_d = {}
    f_endorse_d = {}
    f_threads_d = {}
    f_comments_d = {}

    cin.next()
    for l in cin:
        yob_d = update_num_dict(l[6], yob_d)
        f_post_d = update_num_dict(l[8], f_post_d)
        f_votes_d = update_num_dict(l[9], f_votes_d)
        f_endorse_d = update_num_dict(l[10], f_endorse_d)
        f_threads_d = update_num_dict(l[11], f_threads_d)
        f_comments_d = update_num_dict(l[12], f_comments_d)

    yob_l = dict_to_list(yob_d)
    f_post_l = dict_to_list(f_post_d)
    f_votes_l = dict_to_list(f_votes_d)
    f_endorse_l = dict_to_list(f_endorse_d)
    f_threads_l = dict_to_list(f_threads_d)
    f_comments_l = dict_to_list(f_comments_d)

    yob_map = build_bins(yob_l)
    f_post_map = build_bins(f_post_l)
    f_votes_map = build_bins(f_votes_l)
    f_endorse_map = build_bins(f_endorse_l)
    f_threads_map = build_bins(f_threads_l)
    f_comments_map = build_bins(f_comments_l)

    dump_map(yob_map, ''.join(['yob_map_', fname_ps, '.pkl']))
    dump_map(f_post_map,''.join(['f_post_map_',fname_ps, '.pkl']))
    dump_map(f_votes_map, ''.join(['f_votes_map_', fname_ps, '.pkl']))
    dump_map(f_endorse_map, ''.join(['f_endorsed_map_', fname_ps, '.pkl']))
    dump_map(f_threads_map, ''.join(['f_threads_map_', fname_ps, '.pkl']))
    dump_map(f_comments_map, ''.join(['f_comments_map_', fname_ps, '.pkl']))

    return None


if __name__ == '__main__':
    if len(sys.argv) < 3:
        print 'Usage: python numeric_generalization_v2.py in_file bin_size'
        sys.exit(1)

    fname_in = sys.argv[1]
    fname_out = fname_in[:-4]
    bin_size = int(sys.argv[2])
    #YoB_binsize = bin_size

    f_in = open(fname_in, 'r')
    c_in = csv.reader(f_in)
    create_value_maps(c_in, fname_out)