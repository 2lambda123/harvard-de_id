6/29/15
Changing the buildcountrygeneralizer.py to work with the year two data set. The first change is
to use the cc_by_ip field in the database; this is the country code as determined by the
ip number of the most common login. While this isn't perfect as a locator, it is better than
the self-provided country codes or names, which are nearly always blank.

Moved the global geo_binsize from de_id_functions.py to this program, since this is the only
place that the global is used. It might be useful to make this into a command-line option
rather than having to edit the source code to change the bin size.

Rather than changing the .db file to include the name of the country for each student, we will
use the cc_by_ip field, which is a country code. This can be mapped to the country name by using
the pycountry library. There are other fields that proport to have the country for the student in them,
but these are self-reported and often have no content.

We then build four dictionaries:
    cc_to_countries, which maps from the cc_by_ip field to the name of a country, gotten from pycountry;
    countrydist, which maps from country code to the number of instances with that value
    country2cont, which maps from the name (not code) of a country to the region for that country; this is just
        reading in the values in a table that has already been created and stored in a file
    cont2country, which takes the country2cont table and the country code to country table and does an inverse
    mapping, allowing mapping from a region to the set of country codes


6/21/15

Extracted a main method from buildcountrygeneralizer.py so that it can be called
from another script. The main program will take the names of the files needed
to populate the in-memory structures, open them and build the structures, and then
write the country generalization table to disk. This is the first of two step for
this process (ultimately); the next step will be to extract the code that reads
and writes the files and have the processing done on in-memory structures that can
be passed from program to program by a wrapping script.

Did the same with buildFullSuppressionSet.py; this will also need to be re-factored
to get rid of writing all of the intermediate values to disk; there also needs to be
some review of the information (some of it useful, much of it for debugging) that is
currently printed out by the main routine.

Did the same with courseSetDeIdentify.py, also adding the ability to decide to use
suppression by activity or suppression by random choice to the input parameters.

5/21/15
Added a generalization dictionary for level of education. There will be two categories;
those who have finished at least a bachelor's degree (labelled 'pg') and those who have
not or do not report (labelled 'ug'). This is done with a dictionary that is wired
in to buildDeIddntifedCSV.py, which is not elegant but works.

Added constants in the de_id_functions.py file that define the bin sizes used in the
de-identification. These are YoB_binsize, nforum_post_binsize, and geo_binsize. These can
be changed independently.

The full toolchain is now done; we can generate a de-identified file using a technique
that favors generalization over suppression. The only time we use suppression is if the
set of classes uniquely identifies a student, or if the record when being written out
contains characters that can't be written to a .csv file. We have filters for class
suppression that will pick class/student combinations to suppress at random or based on
favoring suppression of the classes in which the student was the least active. We also have
ways of dynamically determining bins of a minimum size for numeric values and for
geographic locations.

The work needed now is to determine the size of the bins that will lead to a k-anonymous
data set for various values of k. For k = 5 the bins need to be surprisingly large;
currently values of 20,000 lead to more than 55k combinations of values in which 4 or
fewer students reside in the bins.

5/10/15
Time to write the routine that will write out the final, de-identified .csv file.
The fields that we want to write are
    course_id -> course_id
    de-identified student_id -> user_id
    registered -> registered

    viewed -> viewed
    explored -> explored
    certified -> certified
    final-country or region -> final_cc_cname
    Level of education -> LoE
    Year of birth -> YoB
    Gender -> gender
    grade -> grade
    start time -> start_time
    last event -> last_event
    number of events -> nevents
    number of days active -> ndays_act
    number of videos played -> nplay_video
    number of chapters -> nchapters
    number of forum posts -> nforum_posts
    roles -> roles
    incomplete flag -> ???


4/16/15
Simplified the code in edLevelDistribution to only have the list worked on contain the level of education. This
is not quite as simple as it seems, since fetchall() returns a list of tuples, so the resulting list was of
singleton tuples and so still needs to be indexed.

Looked into the data and found that YoB has as possible values both 'NA' and '' for those that hadn't entered
something. Changed buildDB.py to map YoB values of 'NA', those before 1930, and those after 2005 to the value ''.
This will replace the suppression of records with YoB before 1930, which seemed a bit extreme.

Changed the routine in edLevelDistribution that built the dictionary to be more general. It will now build
a dictionary of values/number of records with those values from any list of singleton tuples. You can also
pass in a filter function, which takes a value and can exchange that value for some other, based on whatever
metric you wish. Renamed this function from buildleveldict to builddistdict.

3/19/15
Added code to buildEquivClasses.py to allow the building of the equivalence classes from a .db file as
well as from the .csv.

3/17/15
Back to the code (although some has been generated without adding to the log). Starting by moving all of
the code needed to build the database into buildDB.py, so that it won't clutter up the de_id_functions.py.
Moved splitDate(), idGen2(), and sourceLoad() into the buildDB.py. Also making buildDB.py directly
executable, which should make things easier.

Added a check to buildCourseDict.py to make sure that the courses being added to the dictionary are listed in
a canonical order (I just used dictionary order). It turns out that this makes a difference-- the concatenation
of the classes taken by a student might differ in order only, so putting them in the same order will cut
down on some of the uniqueness.

1/21/15
Starting to build the program that will separately build the database. The thought is to build something that
will take as arguments the .csv file that contains all of the information and the name of the database to output,
and then create something that a version of De-identification.py can use to create a k-anonymous database.

1/29/15
Moved creation of the country/continent table to the buildDB.py function. Have started
to find places whereC some information was just printed out and moved into places where
it is only printed when a verbose option is picked.

1/8/15
Currently, every time that De-identification.py is run, it builds the whole sqlite
database from a .csv file that contains the information on courses. This database
contains two tables; one which is unmodified and one that is used to anonymize the 
data. 

This functionality can, and should, be split into different programs. One will create
the database, containing a (mostly) unmodified version of the csv file (that itself
contains the course and person data) and another that starts by removing the anonymized 
table, copying the original table, and then working on the anonymization.

In de_id_functions.py:
	Needed to install:
		pycountry
		pp
		pygeoip
	
	changed undeclared and unused variable c to cursor in a number of functions; assumed to be a typo
	
in De-identification.py
	Removed a bad space at line 515
	
