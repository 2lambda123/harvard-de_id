4/9/17
Starting to understand the DataFrame...it is a 2-d array, that can be initialized by a list of lists. These can be
built from the .csv file for all of the various numerics that need to be generalized-- this couild be done in a single
pass to make it most efficient, where each list is a value (YoB, or number of posts), followed by a count of the number
of records with that value, followed by the accumulated total (which will be the count X the number).

The question comes up what to do with the marked value (that is, the value that indicates that there is no value given).
This could be handled either as a special case in the bin building code, or be removed before and held separate. I sort
of like the second of these, as it makes the bin building code cleaner, but need to find a nice way to handle that first.
Current theory is that the marked value will be removed from the list of values/counts prior to sending to the bin
builder, then added to the dictionary that the bin builder returns, so that it can be written out.

4/7/17
Looking at what Olivia did in numeric_generalization_v2.py.

The basic datastructure she works with is a pandas DataFrame, which she constructs from a list obtained from performing
a query on the database. The query returns, for year of birth, a list ordered by yob with the yob, the count of records
with that yob, and the sum of the yob.
years.
3/15/17
Thinking more about the cleanup of the data. For some of the forum counts, ther are only relatvely small nunbers of
records for the count at all, and there is a long tail as the counts go up. The long tail starts with the number of
posts, which begins to tail out at 56 posts. The number of comments begins to tail at 54, but the number of
endorsements begins to tail at 1. The current strategy is to simply make all of the tailed values the same as the
lowest value in the tail, but we could try other things (the mean or median come to mind).

For level of education, we will conflate "learn", "Learn", and "null" to a value of unreported (''). We should check
to see if 'learn' should be retained; it is not a value that is documented but may have been added for a special purpose.



3/13/17
Finishing up the code that will extract a .csv file from the large person/course dataset file that will contain only the
fields that are quasi-identifiers. Along with extracting the quasi-identifiers, the values for some of the self-reported
quasi-identifiers is checked, and those that seem to be unlikely to be accurate (like ages under 12 or over 85).

For number of forum posts, if there were more than 57 posts, we will group the students together, since this is a long
tail that in total contains 3,106 students.

for the number of forum comments, if there were more than 54 comments, we will set the number of comments to 55, as
there were a total of 2780 students who were in this category.


1/15/16

Added code to numeric_generalization that will allow passing in the bin sizes as arguments. If no numbers are passed in,
or if they are not integers, the size of the bin will default to the size set in the code (and, if an invalid entry was
passed in, an error message will be printed out.

Note that we still don't deal with the case in which the minimum bin size is too large for the number of entries that we
have; in such a case the bin should be everything.

Updated the pydoc for the functions in numeric_generalization to reflect the new code.

1/12/16

Have made considerable changes to the numeric_generalization.py functions.

Changed findBinEndpoints to assume that the first argument was a list of pairs of (value, count), sorted in ascending
order by value, that need to be binned. If there are non-entry or other forms of errors to deal with, that is done
prior to calling the routine.

Also totaled up the number of entries prior to building the bins, and then when a bin size is reached subtract the
number of entries in the bin from the total number to determine if there are sufficient entries for another bin; if not
the last entries are added to the last bin (and the bin endpoint is changed). The end result is to do the
endpoint determination in one pass rather than two.

Simplified createConversionDict to work on one pass through the set of values to be binned.

1/1/16

Beginning to re-work some of the generalization code that Olivia wrote. First is to change the code for findBinEndpoints
so that it can assume that it is passed a list of integer pairs, where the first entry in each pair is the value to be
generalized, and the second is the number of those values. It is also assumed that the list is in ascending order on the
first field. So for year-of-birth it will be earlier years to later years; for number of forum posts it will be lowest
number of posts to highest. Part of the idea here is to make findBinEndpoints a very general function that doesn't know
what kind of data it is working on, and especially not the special cases (such as values for un-entered data) that it is
trying to deal with. Those will be taken care of at the level of the functions that call this one.

Changed the code to not build a table in the database, but to take the dictionaries that were built mapping the value of
a generalized value to the range/mean and simply pickle that dictionary. This required adding two new command-line arguments,
the names of the files for the pickle.

Also realized that the current generalization code will include records that have previously been determined to be
suppressed by the combination of students/class combination; this may not change any of the results of de-identification,
but it might be interesting to look at what happens if the binning is only done with those records that remain after
this suppression is done.

Currently the code crashes in createConversionDict, to which minimal (no?) changes were made. Need to take a look at what
that function is doing.

12/4/15

Considerable cleanup added to buildDB.py. Changed the building of the country-name from country codes (using the
cc_by_ip field) from being a separate operation that did a 'Select' over the entire db to one where a dictionary
is constructed during the building of the db itself (single pass); then saved that as a pickled dictionary rather than
adding a separate column in to the database for each record. Note that each record already has a country-name associated
with it; that name is different from the one that is given by pycountry in some instances, but only in terms of the
spelling, never in terms of the identification.

Made sure that all of the registered/viewed/explored/certified flags were integer values (explored was stored as floating
point). For number of forum posts, changed the unmarked value to '0' rather than '' or 'NA' so that the binning could be
made simpler.

The date format had changed in such a way that we were not removing the times from the date/times; changed the
splitDate() routine to account for the new format.

Added code to buildDB.py that allows generating data bases for all students; for students who had at least viewed course
content; for students who had at least explored the content; and for only students who had received a certificate.

8/26

Problem found. While the code in numeric_generalization.py creates a table entry for nforum_posts of
'9999', that value is used to mark entries where there were no posts. In the generalization code
this was replaced by the range in the table (73-9999), but in the generation of the .csv file it was replaced
by a blank (along with a blank for the mean). The correct way to deal with this is to replace the value
'9999' with '0' in both places, and have the mean from those values be '0'. Doing this meant more records
were suppressed, but the resulting files were k-anonymous.

8/24

A lot of the code has been simplified, but the failure to anonymize remains. I did build the suppression set without taking
into account the number of forum posts, and the .csv file that was generated off of that suppression set was 5-anon. So
this would indicate that the problem is in the handling of the number of forum posts, which at least narrows it down.

8/22 and 8/23

Olivia wrote to say that the de-identified sets were in fact not 5-anonymous, which I thought they
were. Trying to track down where the problem is.

Checked to make sure that none of the records that are in the full suppression set are being written into the
final CSV file. In fact, none are, so it appears that the buildDeIdentifiedCSV program is working correctly,
writing all and only those records that are not in the full suppression set.

8/20/15

Finished the first round that will de-identify the year2 data sets. Lots more suppression because of identification by
the set of classes, which is interested. Needed to deal with the floating-point appearance of the YoB and nforum posts;
the current solution is to simply slice the last two characters off (since they all end in '.0'); a more general solution
would be to take the str(int(float())) of the value. Maybe soon.

An even better improvement for the buildDeIdentifiedCSV.py program would be to create an object that was the record, and
have methods that would add the calculated fields. There could also be a method that would give the right order to the
fields when things like the mean is added. This would make more sense than the indexing that is currently happenting, which
is both obscure and error prone. This approach might actually make some sense.

8/19/15
After changing Olivia's numeric_generalization code to run as a stand-alone program, the database table that stores
the bins for numbers of forum posts (nforum_posts_bins) was not being saved, even though it used the same code as
was used to generate the bins for Year of Birth (which was being saved). Stepping through the code by hand caused the
table to be saved. Putting in print messages caused the data to be saved. But running the same code stand-alone resulted
in the table not being saved.

A classic Heisenbug; it only appears when you aren't looking. Which means that it was a timing issue.

The real problem was that the program was exiting before the implicit synchronization of the sqlite database when run
stand-alone, but this synchronization was happening in the time it took to either run the program in a python shell or
when there were print statements. The solution was to do an explicit close of the database at the end of the program,
which forces a write of any changes.

I also moved the constants YoB_binsize and nforum_post_binsize to this file, as it is the only one that uses these
constants.

6/29/15
Changing the buildcountrygeneralizer.py to work with the year two data set. The first change is
to use the cc_by_ip field in the database; this is the country code as determined by the
ip number of the most common login. While this isn't perfect as a locator, it is better than
the self-provided country codes or names, which are nearly always blank.

Moved the global geo_binsize from de_id_functions.py to this program, since this is the only
place that the global is used. It might be useful to make this into a command-line option
rather than having to edit the source code to change the bin size.

Rather than changing the .db file to include the name of the country for each student, we will
use the cc_by_ip field, which is a country code. This can be mapped to the country name by using
the pycountry library. There are other fields that proport to have the country for the student in them,
but these are self-reported and often have no content.

We then build four dictionaries:
    cc_to_countries, which maps from the cc_by_ip field to the name of a country, gotten from pycountry;
    countrydist, which maps from country code to the number of instances with that value
    country2cont, which maps from the name (not code) of a country to the region for that country; this is just
        reading in the values in a table that has already been created and stored in a file
    cont2country, which takes the country2cont table and the country code to country table and does an inverse
    mapping, allowing mapping from a region to the set of country codes


6/21/15

Extracted a main method from buildcountrygeneralizer.py so that it can be called
from another script. The main program will take the names of the files needed
to populate the in-memory structures, open them and build the structures, and then
write the country generalization table to disk. This is the first of two step for
this process (ultimately); the next step will be to extract the code that reads
and writes the files and have the processing done on in-memory structures that can
be passed from program to program by a wrapping script.

Did the same with buildFullSuppressionSet.py; this will also need to be re-factored
to get rid of writing all of the intermediate values to disk; there also needs to be
some review of the information (some of it useful, much of it for debugging) that is
currently printed out by the main routine.

Did the same with courseSetDeIdentify.py, also adding the ability to decide to use
suppression by activity or suppression by random choice to the input parameters.

5/21/15
Added a generalization dictionary for level of education. There will be two categories;
those who have finished at least a bachelor's degree (labelled 'pg') and those who have
not or do not report (labelled 'ug'). This is done with a dictionary that is wired
in to buildDeIddntifedCSV.py, which is not elegant but works.

Added constants in the de_id_functions.py file that define the bin sizes used in the
de-identification. These are YoB_binsize, nforum_post_binsize, and geo_binsize. These can
be changed independently.

The full toolchain is now done; we can generate a de-identified file using a technique
that favors generalization over suppression. The only time we use suppression is if the
set of classes uniquely identifies a student, or if the record when being written out
contains characters that can't be written to a .csv file. We have filters for class
suppression that will pick class/student combinations to suppress at random or based on
favoring suppression of the classes in which the student was the least active. We also have
ways of dynamically determining bins of a minimum size for numeric values and for
geographic locations.

The work needed now is to determine the size of the bins that will lead to a k-anonymous
data set for various values of k. For k = 5 the bins need to be surprisingly large;
currently values of 20,000 lead to more than 55k combinations of values in which 4 or
fewer students reside in the bins.

5/10/15
Time to write the routine that will write out the final, de-identified .csv file.
The fields that we want to write are
    course_id -> course_id
    de-identified student_id -> user_id
    registered -> registered

    viewed -> viewed
    explored -> explored
    certified -> certified
    final-country or region -> final_cc_cname
    Level of education -> LoE
    Year of birth -> YoB
    Gender -> gender
    grade -> grade
    start time -> start_time
    last event -> last_event
    number of events -> nevents
    number of days active -> ndays_act
    number of videos played -> nplay_video
    number of chapters -> nchapters
    number of forum posts -> nforum_posts
    roles -> roles
    incomplete flag -> ???


4/16/15
Simplified the code in edLevelDistribution to only have the list worked on contain the level of education. This
is not quite as simple as it seems, since fetchall() returns a list of tuples, so the resulting list was of
singleton tuples and so still needs to be indexed.

Looked into the data and found that YoB has as possible values both 'NA' and '' for those that hadn't entered
something. Changed buildDB.py to map YoB values of 'NA', those before 1930, and those after 2005 to the value ''.
This will replace the suppression of records with YoB before 1930, which seemed a bit extreme.

Changed the routine in edLevelDistribution that built the dictionary to be more general. It will now build
a dictionary of values/number of records with those values from any list of singleton tuples. You can also
pass in a filter function, which takes a value and can exchange that value for some other, based on whatever
metric you wish. Renamed this function from buildleveldict to builddistdict.

3/19/15
Added code to buildEquivClasses.py to allow the building of the equivalence classes from a .db file as
well as from the .csv.

3/17/15
Back to the code (although some has been generated without adding to the log). Starting by moving all of
the code needed to build the database into buildDB.py, so that it won't clutter up the de_id_functions.py.
Moved splitDate(), idGen2(), and sourceLoad() into the buildDB.py. Also making buildDB.py directly
executable, which should make things easier.

Added a check to buildCourseDict.py to make sure that the courses being added to the dictionary are listed in
a canonical order (I just used dictionary order). It turns out that this makes a difference-- the concatenation
of the classes taken by a student might differ in order only, so putting them in the same order will cut
down on some of the uniqueness.

1/21/15
Starting to build the program that will separately build the database. The thought is to build something that
will take as arguments the .csv file that contains all of the information and the name of the database to output,
and then create something that a version of De-identification.py can use to create a k-anonymous database.

1/29/15
Moved creation of the country/continent table to the buildDB.py function. Have started
to find places whereC some information was just printed out and moved into places where
it is only printed when a verbose option is picked.

1/8/15
Currently, every time that De-identification.py is run, it builds the whole sqlite
database from a .csv file that contains the information on courses. This database
contains two tables; one which is unmodified and one that is used to anonymize the 
data. 

This functionality can, and should, be split into different programs. One will create
the database, containing a (mostly) unmodified version of the csv file (that itself
contains the course and person data) and another that starts by removing the anonymized 
table, copying the original table, and then working on the anonymization.

In de_id_functions.py:
	Needed to install:
		pycountry
		pp
		pygeoip
	
	changed undeclared and unused variable c to cursor in a number of functions; assumed to be a typo
	
in De-identification.py
	Removed a bad space at line 515
	
